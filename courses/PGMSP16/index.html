<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<title>CS6424/ECE6424: Probabilistic Graphical Models and Structured Prediction</title>
<link href="./style.css" rel="stylesheet" type="text/css">

</head>

<body>

<h1>Probabilistic Graphical Models and Structured Prediction<br>
CS6424/ECE6424 Spring 2016</h1>

<h2 id="announcements">Announcements</h2>

<ul>
    <li>Selected project writeups from the students are available <a href="./project_reports.html">here</a></li>
    <li>Project deliverables detailed <a href="./project_deliverables.html">here</a>.</li>
    <li>Fill out your panel participation preferences <a href="https://docs.google.com/forms/d/1FLrHdfFpLHdIctg1CzNDYPgt2KTEGeexJyravJyhoM4">here</a>. If you want preferences to be counted, you must complete the form before class on Tuesday 1/26. Otherwise, I will treat your bids as "willing" for all sessions.</li>
    <li> Take the Homework 0 Quiz on the course Canvas site (<a href="https://canvas.vt.edu/courses/20053">https://canvas.vt.edu/courses/20053</a>) to see if your prerequisite knowledge aligns with the expectations for this course. The Canvas site will remain open to anyone with a VT login until enrollments are finalized. </li>
    <li>If you are interested in the course and were unable to pre-register, fill out the form <a href="https://docs.google.com/forms/d/1qppk61n0f4PVPzZXgDXRNTXeo4t6zI28XGPthkLp2TU">here</a> to request a force add. I will do my best to include you, but the room capacity limit may prevent everyone from getting in, and there is a limit to the number of students that can effectively learn together in a seminar course.</li>
</ul>


<h2 id="description">Description</h2>

<p>In this course, we will study the wide variety of research on probabilistic graphical models and structured prediction. These areas of machine learning research cover algorithmic approaches to learning and prediction in settings with dependencies in data, variables, and outputs. We will examine the currently understood methods for efficiently managing the complexities that arise when we consider these dependencies in models, study algorithms for reasoning and predicting using these models, and explore how other algorithms can learn parameters and structure from data.</p>

<p>The class will be a shared learning experience, where the students will lead discussions as we work through some seminal texts and recent papers.</p>

<p>Class meets Tuesday and Thursday from 3:30 PM to 4:45 PM in McBryde 232.</p>

<ul>
<li><b>Instructor:</b><br>
    <a href="http://berthuang.com">Bert Huang</a>, Assistant Professor of Computer Science<br>
    Office hours: Thursday 2-3:15 PM, Torgersen Hall 3160L<br>
    <a href="mailto:bhuang@vt.edu">bhuang@vt.edu</a></li>
</ul>

<p>The course homepage is <a href="http://people.cs.vt.edu/~bhuang/courses/pgmsp16/">http://people.cs.vt.edu/~bhuang/courses/pgmsp16/</a> or equivalently <a href="http://berthuang.com/courses/pgmsp16/">http://berthuang.com/courses/pgmsp16/</a>.</p>

<h2 id="goals">Topics and Goals</h2>


<p>Topics include directed models (Bayes nets), undirected models (Markov/conditional random fields), exact inference (junction tree), approximate inference (belief propagation, dual decomposition), parameter learning (MLE, MAP, EM, max-margin), and structure learning. </p>

<p>Having successfully completed this course, the student will be able to:</p>

<ul>
<li>Analyze and contrast broad classes of probabilistic graphical models (Bayes nets vs. Markov nets; directed vs. undirected graphical models)</li>
<li>State and describe independence assumptions encoded by different models</li>
<li>Explain and implement algorithms for learning and inference (e.g., belief propagation)</li>
<li>Discuss and critique research papers on these topics</li>
<li>Identify open research questions in these areas</li>
</ul>

<h2 id="prerequisites">Prerequisites</h2>

<p>The listed prerequisite courses cover relevant material that includes introductory-level knowledge of machine learning and statistical learning (likelihood, maximum likelihood, bias and variance, underfitting, overfitting, regularization, cross-validation), probability and statistics, and algorithms.</p>

<p>Please speak with the instructor if you are concerned about your background. Note: If any student needs special accommodations because of any disabilities, please contact the instructor during the first week of classes.</p>

<h2 id="readingandmaterials">Reading and Materials</h2>

This class will be taught using a combination of materials. The main two texts we will use are seminal texts in the field of machine learning. One is a textbook, and the other is a long journal paper, which is available for free.
<ul>
<li><i>Probabilistic Graphical Models: Principles and Techniques</i>, by Daphne Koller and Nir Friedman. <a href="http://pgm.stanford.edu/">http://pgm.stanford.edu</a><br></li>
<li><i>Graphical Models, Exponential Families, and Variational Inference</i>, by Martin Wainwright and Michael Jordan. <a href="https://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">https://www.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf</a>. <br> </li>
</ul>

<h2 id="gradingbreakdown">Grading Breakdown</h2>

<ul>
<li>10%: Discussion leading</li>
<li>30%: Discussion panel participation</li>
<li>30%: Reading summaries</li>
<li>30%: Semester project</li>
</ul>

<h2 id="attendance">Format and Attendance</h2>

<p>After the first two weeks of the semester, the class will center around discussion. We will work through the reading materials together, and you will lead discussion and participate in panels during class sessions. In each class session, you will have one of three roles: <b>panel leader</b>, <b>panelist</b>, or <b>audience</b>. </p>

<ul>

<li>Before class, <b>all students</b> will read the assigned text and submit <i>summaries and questions</i> about the text. The questions will be available for the rest of the class to see to aid discussion.</li>

<li>During class, the <b>panel leaders</b> will guide the discussion along with the instructor. They will first briefly summarize the main points from the reading, and begin working through the text, asking questions to the panelists and to the audience. These questions may come from the questions posed by the class from their summaries and questions.</li>

<li>The <b>panelists</b> will be the main participants in the discussion. Panelists will be free to talk at any reasonable time and will be able to have a free-flowing conversation about the ideas in the text.</li>

<li>The <b>audience</b> members will also be able to join the discussion, but to make a large, discussion-based course an effective learning experience for all of us, we will require that audience members be called on by a panel leader to join the conversation.</li>

<li><b>Update 2/16/16:</b> Since the class has reduced in size to be manageable as a full-class discussion, we will not use the panel structure anymore. Instead, the leader will lead the full class in the discussion of the reading.</li>

</ul>

<p>The goal of each class is to understand the ideas presented in the assigned reading as much as is possible. We will work through examples, work together to clarify points of confusion, and discuss the wider implications of the topics from the reading. To make sure this experience works for everyone, class attendance is mandatory. I will take attendance and penalize unexused absences at my discretion. </p>

<p>In the first week of class, you will rank dates for when to serve as panel leaders or panelists, and I will work out a schedule that satisfies your preferences as much as possible. Keep in mind that since every student must do the reading, serving as a panelist or leader is not much more work on top of the reading. </p>

<h2 id="summaries">Summaries and Questions</h2>

<p>For each assigned reading, you will submit a writeup in your own words that contains two major components: a summary and at least two questions.</p>

<p>The <b>summary</b> should be 1-2 paragraphs describing what main ideas were presented in the text. You should cover the takeaway points, not the fine details.</p>

<p>You will then write (at least) two <b>questions</b>. The first will be a clarification question. What idea in the text was unclear to you? What do you want to understand better? For example, you might write, "Why is Lemma 3, which is presented without proof, guaranteed to be true?" In theory, clarification questions should have answers that we can attempt to find together as a class.</p>

<p>The second <b>question</b> you will include is a discussion question. You might ask about research ideas, how the technique we studied can be applied, or why a mathematical concept is important. In theory, a discussion question may not have a true answer, but we can speculate and brainstorm about them during our discussion.</p>

<p>You should submit your summary and questions on Canvas as posts in the discussion thread for the class session. These threads are set so you cannot see other posts until you post your own summary and questions.</p>

<h2 id="project">Project</h2>

<p>For the class project, you will conduct research with the goal of producing findings worthy of publication at a conference. The project should be done in groups of 1&ndash;3 students, and should feature a novel algorithmic contribution to some aspect of learning, inference, prediction, analysis, or application using probabilistic graphical models or structured predictors. You are strongly encouraged to incorporate this project with any other research you are working on, and you are welcome to include collaborators outside the class as long as you are doing a substantial proportion of the research yourself.</p>

<p>You will first write a 1-page proposal on your project before Spring Break. You are highly encouraged to begin brainstorming about topics as soon as possible. You will write a 6&ndash;10 page paper on your findings due at the end of the semester, reporting your contribution, background material, evaluation (experiments and/or analysis), and conclusions. Details on the proposal and paper details are available at <a href="./project_deliverables.html">./project_deliverables.html</a>.</p>

<p>Here is a <a href="./project_ideas.html">list</a> of brainstormed project ideas to help you design your own project. See additional project ideas from similar courses, such as from <a href="https://docs.google.com/document/u/1/d/1jxAcUZdGXAkD7TNxYoeR6W5w_4wNJr5DhPgSc7K3WN0/pub">CMU</a>, <a href="http://www.cis.upenn.edu/~taskar/courses/cis620-sp09/project.html">UPenn</a>, and the projects from the previous PGM course at <a href="https://filebox.ece.vt.edu/~s14ece6504/">VT</a> for more ideas.</p>

<h2 id="resources">External Resources</h2>

<p>Various related courses have valuable materials that can help supplement our official reading. Some of these are linked below.</p>

<ul>

<li>Dhruv Batra's PGM course from 2014 <a href="https://filebox.ece.vt.edu/~s14ece6504/">https://filebox.ece.vt.edu/~s14ece6504/</a></li>
<li>Daphne Koller's Coursera PGM course <a href="https://www.coursera.org/course/pgm">https://www.coursera.org/course/pgm</a></li>
<li>David Sontag's PGM course from 2013 <a href="http://cs.nyu.edu/~dsontag/courses/pgm13/">http://cs.nyu.edu/~dsontag/courses/pgm13/</a></li>
</ul>

<h2 id="academicintegrity">Academic Integrity</h2>

<p>The tenets of the Virginia Tech Graduate Honor Code will be strictly enforced in this course, and all assignments shall be subject to the stipulations of the Graduate Honor Code. For more information on the Graduate Honor Code, please refer to the GHS Constitution at <a href="http://ghs.graduateschool.vt.edu">http://ghs.graduateschool.vt.edu</a>.</p>

<p>This course will have a zero-tolerance policy regarding plagiarism or other forms of cheating. Your homework assignments must be your own work, and any external source of code, ideas, or language must be cited to give credit to the original source. I will not hesitate to report incidents of academic dishonesty to the graduate school.</p>


<h2 id="schedule">Schedule</h2>

<div style="overflow-x:auto; width:100%">
<table style="width:95%; margin: 0 auto;">
    <thead>
        <tr>
            <th>Session</th>
            <th>Date</th>
            <th>Topic</th>
            <th>Reading</th>
            <th>Panelists (Leaders in <b>bold</b>)</th>
        </tr>
    </thead>
    <tbody>

        <tr>
        	<td>1</td>
        	<td>Tue&nbsp;1/19</td>
        	<td>Overview of Graphical Models and Structured Prediction</td>
        	<td>KF 1, 2<br />
         Why ML Needs Statistics by Welling (<a href ="http://www.ics.uci.edu/~welling/publications/papers/WhyMLneedsStatistics.pdf">link</a>)</td>
        	<td>None</td>
        </tr>
        <tr>
        	<td>2</td>
        	<td>Thu&nbsp;1/21</td>
        	<td>Bayesian Networks</td>
        	<td>KF 3-3.2</td>
        	<td>None</td>
        </tr>
        <tr>
        	<td>3</td>
        	<td>Tue&nbsp;1/26</td>
        	<td>Bayesian Networks</td>
        	<td>KF 3.3-3.5</td>
        	<td>None</td>
        </tr>
        <tr>
        	<td>4</td>
        	<td>Thu&nbsp;1/28</td>
        	<td>Undirected Graphical Models</td>
        	<td>KF 4-4.4</td>
        	<td>None</td>
        </tr>
        <tr>
        	<td>5</td>
        	<td>Tue&nbsp;2/2</td>
        	<td>Undirected Graphical Models</td>
        	<td>KF 4.5-4.7</td>
        	<td><b>Tianyi Li</b>, Jason Louis Granstedt, William Ryan Doan</td>
        </tr>
        <tr>
        	<td>6</td>
        	<td>Thu&nbsp;2/4</td>
        	<td>Exact Inference: Variable Elimination</td>
        	<td>KF 9-9.4, 9.7</td>
        	<td><b>Sirui Yao</b>, Abhilash Chowdhary, Aishwarya Agrawal, Arjun Chandrasekaran</td>
        </tr>
        <tr>
        	<td>7</td>
        	<td>Tue&nbsp;2/9</td>
        	<td>Exact Inference: Clique Trees</td>
        	<td>KF 10</td>
        	<td><b>Arijit Ray</b>, Aishwarya Agrawal, Arjun Chandrasekaran</td>
        </tr>
        <tr>
        	<td>8</td>
        	<td>Thu&nbsp;2/11</td>
        	<td>Inference as Optimization</td>
        	<td>KF 11-11.3</td>
        	<td><b>Bert Huang</b>, Aishwarya Agrawal, Viresh Ranjan, Xuan Zhang</td>
        </tr>
        <tr>
        	<td>9</td>
        	<td>Tue&nbsp;2/16</td>
        	<td>Inference as Optimization</td>
        	<td>KF 11.4-11.6</td>
        	<td><b>Bert Huang</b></td>
        </tr>
        <tr>
        	<td>10</td>
        	<td>Thu&nbsp;2/18</td>
        	<td>MAP Inference</td>
        	<td>KF 13-13.5</td>
        	<td><b>Yi Tian</b></td>
        </tr>
        <tr>
        	<td>11</td>
        	<td>Tue&nbsp;2/23</td>
        	<td>Parameter Learning in Bayesian Networks</td>
        	<td>KF 17</td>
        	<td><b>Abhilash Chowdhary</b></td>
        </tr>
        <tr>
        	<td>12</td>
        	<td>Thu&nbsp;2/25</td>
        	<td>Structure Learning in Bayesian Networks</td>
        	<td>KF 18</td>
        	<td><b>Bijaya Adhikari</b></td>
        </tr>
        <tr>
        	<td>13</td>
        	<td>Tue&nbsp;3/1</td>
        	<td>Partially Observed Data</td>
        	<td>KF 19.1-19.2</td>
        	<td><b>Jiali Lin</b></td>
        </tr>
        <tr>
        	<td>14</td>
        	<td>Thu&nbsp;3/3</td>
        	<td>Learning Undirected Models</td>
        	<td>KF 20</td>
        	<td><b>Bert Huang</b></td>
        </tr>
        <tr>
        	<td></td>
        	<td></td>
        	<td>Spring Break</td>
        	<td></td>
        	<td></td>
        </tr>
        <tr>
        	<td>15</td>
        	<td>Tue&nbsp;3/15</td>
        	<td>Graphical Models as Exponential Families</td>
        	<td>WJ 3</td>
        	<td><b>Elaheh Raisi</b></td>
        </tr>
        <tr>
        	<td>16</td>
        	<td>Thu&nbsp;3/17</td>
        	<td>Sum-Product, Bethe-Kikuchi, Expectation-Propagation</td>
        	<td>WJ 4</td>
        	<td><b>Aroma Mahendru</b></td>
        </tr>
        <tr>
        	<td>17</td>
        	<td>Tue&nbsp;3/22</td>
        	<td>Variational Methods in Parameter Estimation</td>
        	<td>WJ 6</td>
        	<td><b>Viresh Ranjan</b></td>
        </tr>
        <tr>
        	<td>18</td>
        	<td>Thu&nbsp;3/24</td>
        	<td>Convex Relaxations and Upper Bounds</td>
        	<td>WJ 7</td>
        	<td><b>Bert Huang</b></td>
        </tr>
        <tr>
        	<td>19</td>
        	<td>Tue&nbsp;3/29</td>
        	<td>Dual Decomposition for Inference</td>
        	<td>Introduction to Dual Decomposition for Inference by Sontag et al. (<a href ="http://cs.nyu.edu/~dsontag/papers/SonGloJaa_optbook.pdf">link</a>)</td>
        	<td><b>Arjun Chandrasekaran</b></td>
        </tr>
        <tr>
        	<td>20</td>
        	<td>Thu&nbsp;3/31</td>
        	<td>Structured SVM</td>
        	<td>Cutting Plane Training of Structural SVMs by Joachims et al. (<a href ="http://www.cs.cornell.edu/People/tj/publications/joachims_etal_09a.pdf">link</a>)</td>
        	<td><b>Jinwoo Choi</b></td>
        </tr>
        <tr>
        	<td>21</td>
        	<td>Tue&nbsp;4/5</td>
        	<td>Max-Margin Markov Networks and Inner-Dual Learning</td>
        	<td>Max-Margin Markov Networks by Taskar et al. (<a href ="http://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf">link</a>) <br />
         Learning Structured Prediction Models: A Large Margin Approach by Taskar et al. (<a href ="http://homes.cs.washington.edu/~taskar/pubs/icml05.pdf">link</a>)</td>
        	<td><b>Shuangfei Fan</b></td>
        </tr>
        <tr>
        	<td>22</td>
        	<td>Thu&nbsp;4/7</td>
        	<td>Inner-dual Learning with Dual Decomposition</td>
        	<td>Learning Efficiently with Approximate Inference via Dual Losses by Meshi et al. (<a href ="http://ttic.uchicago.edu/~meshi/papers/587.pdf">link</a>) <br />
         A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction by Hazan and Urtasun (<a href ="http://papers.nips.cc/paper/3913-a-primal-dual-message-passing-algorithm-for-approximated-large-scale-structured-prediction.pdf">link</a>)</td>
        	<td><b>Bert Huang</b></td>
        </tr>
        <tr>
        	<td>23</td>
        	<td>Tue&nbsp;4/12</td>
        	<td>Markov Logic Networks</td>
        	<td>Markov Logic Networks by Richardson and Domingos (<a href ="http://link.springer.com/article/10.1007/s10994-006-5833-1">link</a>)</td>
        	<td><b>Chris Dusold</b></td>
        </tr>
        <tr>
        	<td>24</td>
        	<td>Thu&nbsp;4/14</td>
        	<td>Hinge-Loss MRFs and Probabilistic Soft Logic</td>
        	<td>Hinge-Loss Markov Random Fields and Probabilistic Soft Logic by Bach et al. (<a href ="http://arxiv.org/abs/1505.04406">link</a>). Read Sections 1, 2, 3, 5, 6-6.3. Optionally skim the rest. </td>
        	<td><b>Walid Chaabene</b></td>
        </tr>
        <tr>
        	<td>25</td>
        	<td>Tue&nbsp;4/19</td>
        	<td>Latent-Variable Structured SVM</td>
        	<td>Learning Structural SVMs with Latent Variables by Yu et al. (<a href ="http://www.cs.cornell.edu/~cnyu/papers/icml09_latentssvm.pdf">link</a>) <br />
         Marginal Structured SVM with Hidden Variables by Ping et al. (<a href ="http://www.ics.uci.edu/~ihler/papers/icml14.pdf">link</a>)</td>
        	<td><b>Bert Huang</b></td>
        </tr>
        <tr>
        	<td>26</td>
        	<td>Thu&nbsp;4/21</td>
        	<td>Latent-Variable Inner-Dual methods</td>
        	<td>Efficient Structured Prediction with Latent Variables for General Graphical Models by Schwing et al. (<a href ="http://alexander-schwing.de/papers/SchwingEtAl_ICML2012.pdf">link</a>) <br />
         Paired-Dual Learning for Fast Training of Latent Variable Hinge-Loss MRFs by Bach et al. (<a href ="http://jmlr.org/proceedings/papers/v37/bach15.pdf">link</a>)</td>
        	<td><b>Subhodip Biswas</b></td>
        </tr>
        <tr>
        	<td>27</td>
        	<td>Tue&nbsp;4/26</td>
        	<td>Learning Deep Structured Models</td>
        	<td>Learning Deep Structured Models by Chen et al. (<a href ="http://alexander-schwing.de/papers/ChenSchwingEtAl_ICML2015.pdf">link</a>)</td>
        	<td><b>William Ryan Doan</b></td>
        </tr>
        <tr>
        	<td>28</td>
        	<td>Thu&nbsp;4/28</td>
        	<td>Causality</td>
        	<td>KF 21</td>
        	<td><b>Jason Louis Granstedt</b></td>
        </tr>
        <tr>
        	<td>29</td>
        	<td>Tue&nbsp;5/3</td>
        	<td>Wrap Up</td>
        	<td></td>
        	<td>Bert Huang</td>
        </tr>

    </tbody>
</table>
</div>

<h2></h2>

<p>Disclaimer: This syllabus details the plans for the course, which are subject to change. I will make sure any changes are clearly announced and will always be intended for your benefit.</p>

<p>For visitors outside the course: You are welcome to use the course materials for educational purposes. Do not sell any of this content.</p>



</body>
</html>
