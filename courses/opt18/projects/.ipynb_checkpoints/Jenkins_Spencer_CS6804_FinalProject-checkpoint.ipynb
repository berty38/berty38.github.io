{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Visualizing Deep Neural Networks and Optimization Paths</center>\n",
    "## <center>Spencer Jenkins</center>\n",
    "### <center>CS 6804: Optimization in Machine Learning - Final Project</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is a preliminary implementation of some of the techniques found in the recent paper by Li, et al. [(Visualizing the Loss Landscape of Neural Nets)](https://arxiv.org/abs/1712.09913). We seek to be able to visualize high-dimensional, non-convex objective functions that appear when using a deep neural network. We would also like to be able to visualize the paths that various optimization algorithms take through this loss surface. \n",
    "\n",
    "We do this by picking two direction vectors of the same dimension as the parameter vector of the network. These two vectors become the axes for our plots. These are taken as standard normal vectors in this implementation. To capture the path of the optimization algorithm, we can not use random vectors. Instead, we take our direction vectors to be the first two PCA components of the matrix of differences between the final network parameters and the network parameters at each step of the optimization process. We let the user determine which plotting procedure to use by changing the <i>showPath</i> variable.\n",
    "\n",
    "The user can change the size and shape of the network layers as well as the optimization algorithm and its associated hyperparameters. This allows the user to visually experiment with changing the approach to their problem. Currently, the dataset being used is the MNIST dataset contained in the sklearn.datasets library. \n",
    "\n",
    "### Future Work\n",
    "As stated, this project is preliminary. The original paper did not provide a code repository, so this is an original implementation of the ideas presented in the paper. Thus, we are using a (relatively) small feed-forward network.\n",
    "\n",
    "The largest difference between this implementation and the paper lies in the scaling of the direction vectors used for plotting. In the paper, a filter-wise normalization was proposed. Because we are not using a convolutional neural network, we do not perform a filter-wise scaling. We chose to scale the direction vectors to the magnitude of the vector containing all the parameters of the network. When plotting the path on the objective function, we do not perform scaling. Eventually, I would like to perform filter-wise scaling for both plotting approachs (with and without the optimization path).\n",
    "\n",
    "Secondly, the resolution of the plots obtained in this implementation are less than those in the paper. This is due to computing constraints. On 4 GPUs, some of the authors' low resolution plots took around an hour to produce! Some consideration into the computational efficiency of the algorithms used might be a helful future step.\n",
    "\n",
    "Lastly, we are considering a MLPClassifier from scikit-learn as our network. This made things easier to set up, as well as kept the number of parameters manageable to allow succesful plotting of the objective function. Eventually, it would be nice to allow the user to interface with a larger, more powerful model (e.g., a network created in TensorFlow). Though this would be more computationally taxing, it would probably provide more practical benefit to the user. \n",
    "\n",
    "Overall, however, the ability to visualize the objective function and the path of your optimization algorithm through it is of great value to understanding and successfully applying these deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# USER SETTINGS \n",
    "# verbose: print statements for debugging\n",
    "# iterations: number of training iterations\n",
    "# showPath: display the optimization path on the objective function plot\n",
    "verbose = True\n",
    "iterations = 25 #50\n",
    "showPath = True #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Import MNIST data\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "# Split data into training and test sets\n",
    "mnist.data, mnist.target = shuffle(mnist.data, mnist.target, random_state = 0)\n",
    "trainData = mnist.data[0:60000,:]/255.\n",
    "trainLabels = mnist.target[0:60000]\n",
    "testData = mnist.data[60000:,:]/255.\n",
    "testLabels = mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.27458428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bert/anaconda/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.97659630\n",
      "Iteration 3, loss = 1.56892341\n",
      "Iteration 4, loss = 1.23871473\n",
      "Iteration 5, loss = 1.01745649\n",
      "Iteration 6, loss = 0.84916384\n",
      "Iteration 7, loss = 0.72800790\n",
      "Iteration 8, loss = 0.64138160\n",
      "Iteration 9, loss = 0.57607383\n",
      "Iteration 10, loss = 0.52464105\n",
      "Iteration 11, loss = 0.48533431\n",
      "Iteration 12, loss = 0.45660109\n",
      "Iteration 13, loss = 0.43532459\n",
      "Iteration 14, loss = 0.41924716\n",
      "Iteration 15, loss = 0.40661407\n",
      "Iteration 16, loss = 0.39579202\n",
      "Iteration 17, loss = 0.38646310\n",
      "Iteration 18, loss = 0.37826508\n",
      "Iteration 19, loss = 0.37126972\n",
      "Iteration 20, loss = 0.36493746\n",
      "Iteration 21, loss = 0.35861693\n",
      "Iteration 22, loss = 0.35323892\n",
      "Iteration 23, loss = 0.34764832\n",
      "Iteration 24, loss = 0.34293090\n",
      "Iteration 25, loss = 0.33812171\n"
     ]
    }
   ],
   "source": [
    "# Train network one iteration at a time, saving weights for trajectory visualization\n",
    "\n",
    "# NOTE: change optimization algorithm by adjusting the 'solver' parameter\n",
    "# NOTE: the number and shapes of the network's hidden layers can be \n",
    "#       modified through the 'hidden_layer_sizes' parameter\n",
    "network = MLPClassifier(hidden_layer_sizes = (10,10,10,), max_iter = 1, solver='sgd', \\\n",
    "                        learning_rate='constant', warm_start=True, verbose = True)\n",
    "weights = []\n",
    "intercepts = []\n",
    "params = []\n",
    "for i in range(0,iterations):\n",
    "    network.fit(trainData,trainLabels)\n",
    "    w = network.coefs_[0]\n",
    "    for i in range(1,len(network.coefs_)):\n",
    "        w = np.append(w,network.coefs_[i])\n",
    "    inter = network.intercepts_[0]\n",
    "    for i in range(1,len(network.intercepts_)):\n",
    "        inter = np.append(inter,network.intercepts_[i])\n",
    "    params.append(np.reshape(np.append(w,inter),(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saves the final weights and intercepts of the network \n",
    "opt_coeffs = copy.deepcopy(network.coefs_)\n",
    "opt_intercepts = copy.deepcopy(network.intercepts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0)\n"
     ]
    }
   ],
   "source": [
    "# Determine the accuracy of the network on the test data\n",
    "accuracy = np.sum(network.predict(testData) == testLabels)/len(testLabels)\n",
    "if verbose:\n",
    "    print('Accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saves the network attributes at the completion of training into a single parameter\n",
    "# vector. The shapes and lengths of these attributes are also stored for later use\n",
    "wVals = []\n",
    "wShapes = []\n",
    "wLens = []\n",
    "bVals = []\n",
    "bShapes = []\n",
    "bLens = []\n",
    "for i in range(0,len(network.coefs_)):\n",
    "    wVals.append(np.reshape(network.coefs_[i],(-1,1)))\n",
    "    wShapes.append(network.coefs_[i].shape)\n",
    "    wLens.append(len(wVals[i]))\n",
    "for i in range(0,len(network.intercepts_)):\n",
    "    bVals.append(np.reshape(network.intercepts_[i],(-1,1)))\n",
    "    bShapes.append(network.intercepts_[i].shape)\n",
    "    bLens.append(len(bVals[i]))\n",
    "w = np.reshape(network.coefs_[0],(-1,1))\n",
    "for i in range(1,len(wVals)):\n",
    "    w = np.reshape(np.append(w,network.coefs_[i]),(-1,1))\n",
    "for i in range(0,len(bVals)):\n",
    "    w = np.reshape(np.append(w,network.intercepts_[i]),(-1,1))\n",
    "w_length = len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determines the differences at each optimization step between the current and\n",
    "# final network parameters\n",
    "differences = np.zeros((w_length,len(params)-1))\n",
    "for i in range(0,len(params)-1):\n",
    "    differences[:,i]=np.reshape((params[-1]-params[i]),w_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform PCA on the matrix of differences computed above. \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(np.transpose(differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine the vectors defining the axes of the visualization.\n",
    "# If showPath, we take the vectors to be the PCA components determined above.\n",
    "# If not showPath, we take random normal vectors and scale them to the norm of \n",
    "# the final parameter vector.\n",
    "if showPath:\n",
    "    d0 = np.reshape(pca.components_[0],(-1,1))\n",
    "    d1 = np.reshape(pca.components_[1],(-1,1))\n",
    "else:\n",
    "    d0 = np.random.randn(w_length,1)\n",
    "    d0 = d0*np.linalg.norm(params[-1])/np.linalg.norm(d0)\n",
    "    d1 = np.random.randn(w_length,1)\n",
    "    d1 = d1*np.linalg.norm(params[-1])/np.linalg.norm(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transforms the parameter vectors along the optimization path into the PCA space\n",
    "transformed = []\n",
    "for i in range(0,len(params)):\n",
    "    transformed.append(pca.transform(np.transpose(params[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transforms the final parameter vector into the PCA space\n",
    "w_star_pca = pca.transform(np.transpose(params[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determines the distance between each parameter vector along \n",
    "# the optimization path and the final parameter vector\n",
    "plotPath = []\n",
    "for i in range(0,len(transformed)):\n",
    "    plotPath.append(transformed[i] - w_star_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determines the coordinates of the optimization path for plotting\n",
    "x_plot = []\n",
    "y_plot = []\n",
    "for i in range(0,len(transformed)):\n",
    "    x_plot.append(plotPath[i][0][1])\n",
    "    y_plot.append(plotPath[i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Determine the loss values within a specified range of the final parameters, traveling \n",
    "# along the previously computed direction vectors\n",
    "\n",
    "# Determine the range:\n",
    "if showPath:\n",
    "    maxPath = np.max(plotPath)\n",
    "    minPath = np.min(plotPath)\n",
    "    boundary = max(abs(maxPath),abs(minPath))+0.2*max(abs(maxPath),abs(minPath))\n",
    "    optimalRate = boundary*2/10\n",
    "else:\n",
    "    boundary = 10\n",
    "    optimalRate = 2\n",
    "\n",
    "# NOTE: optimalRate determines the \"resolution\" of the plot. A smaller number will result in\n",
    "# a higher resolution, but will take longer to run\n",
    "\n",
    "alpha = np.arange(boundary,-boundary,-optimalRate)\n",
    "beta = np.arange(-boundary,boundary,optimalRate)\n",
    "losses = np.zeros((len(alpha),len(beta)))\n",
    "\n",
    "# Determine the loss values:\n",
    "for b in range(0, len(beta)):\n",
    "    for a in range(0, len(alpha)):\n",
    "        w_current = params[-1] + alpha[a]*d0 + beta[b]*d1\n",
    "        network.coefs_[0] = np.reshape(w_current[0:wLens[0]],wShapes[0])\n",
    "        for i in range(1,len(network.coefs_)):\n",
    "            network.coefs_[i] = np.reshape(w_current[np.sum(wLens[0:i]):np.sum(wLens[0:i+1])],wShapes[i])\n",
    "        network.intercepts_[0] = np.reshape(w_current[np.sum(wLens):np.sum(wLens)+bLens[0]],bShapes[0])\n",
    "        for i in range(1,len(network.intercepts_)):\n",
    "            network.intercepts_[i] = np.reshape(w_current[np.sum(wLens)+np.sum(bLens[0:i]):np.sum(wLens)+np.sum(bLens[0:i+1])],bShapes[i])\n",
    "        \n",
    "        predicted = network.predict_proba(trainData)\n",
    "        losses[a,b] = log_loss(trainLabels,predicted)\n",
    "        if verbose:\n",
    "            print (b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the objective function (and optimization path)\n",
    "\n",
    "# Note: the third argument to the plt.contour function is the number of levels to display. If\n",
    "# you find your plot doesn't contain enough information or is obscured by too much of it, try adjusting this number\n",
    "if showPath:\n",
    "    plt.contour(np.arange(-boundary,boundary,optimalRate),np.arange(-boundary,boundary,optimalRate),losses,250)\n",
    "    plt.plot(x_plot[0:],y_plot[0:],'r-')\n",
    "else:\n",
    "    plt.contour(np.arange(-boundary,boundary,optimalRate),np.arange(-boundary,boundary,optimalRate),losses,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
